{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f023a0c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63433a38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìò MLOps Project 1 ‚Äî DistilBERT MRPC Tuning\n",
    "# Week 2+ setup (supports Bonus Task)\n",
    "# ============================================\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, AutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Initialize Weights & Biases ---\n",
    "wandb.login()\n",
    "PROJECT_NAME = \"MLOPS_p1_distilbert\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Metric ---\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "# --- Short, descriptive run name ---\n",
    "def make_run_name(cfg):\n",
    "    \"\"\"Generate concise run name with key hyperparameters.\"\"\"\n",
    "    return f\"lr{cfg['learning_rate']}_wd{cfg['weight_decay']}_wr{cfg['warmup_ratio']}\"\n",
    "\n",
    "# --- Metric computation for Trainer ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_model(config=None):\n",
    "    \"\"\"Train DistilBERT on MRPC with given hyperparameters.\"\"\"\n",
    "    # Normalize config object\n",
    "    config_dict = dict(config) if not isinstance(config, dict) else config\n",
    "\n",
    "    # Generate descriptive run name\n",
    "    run_name = make_run_name(config_dict)\n",
    "\n",
    "    with wandb.init(project=PROJECT_NAME, name=run_name, config=config_dict):\n",
    "        config = wandb.config\n",
    "\n",
    "        # --- Load data ---\n",
    "        dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "        encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        # --- Model setup ---\n",
    "        model_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model_config.num_labels = 2\n",
    "        model_config.hidden_dropout_prob = config_dict[\"classifier_dropout\"]\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\",\n",
    "            config=model_config,\n",
    "        )\n",
    "\n",
    "        # --- Training Arguments ---\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",  # Save each epoch for Bonus Task plots\n",
    "            save_total_limit=1,\n",
    "            learning_rate=config_dict[\"learning_rate\"],\n",
    "            weight_decay=config_dict[\"weight_decay\"],\n",
    "            per_device_train_batch_size=config_dict[\"per_device_train_batch_size\"],\n",
    "            gradient_accumulation_steps=config_dict[\"gradient_accumulation_steps\"],\n",
    "            num_train_epochs=3,\n",
    "            lr_scheduler_type=config_dict[\"lr_scheduler_type\"],\n",
    "            warmup_ratio=config_dict[\"warmup_ratio\"],\n",
    "            optim=config_dict[\"optimizer_type\"],\n",
    "            logging_steps=50,\n",
    "            report_to=\"wandb\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "        )\n",
    "\n",
    "        # --- Trainer ---\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=encoded_dataset[\"train\"],\n",
    "            eval_dataset=encoded_dataset[\"validation\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        # --- Train & Evaluate ---\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # Log final results\n",
    "        wandb.log(eval_results)\n",
    "        return eval_results\n",
    "\n",
    "# ============================================\n",
    "# Sweep Configuration for Week 2\n",
    "# ============================================\n",
    "\n",
    "default_config = {\n",
    "    \"optimizer_type\": \"adamw_torch\",\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"classifier_dropout\": 0.1,\n",
    "}\n",
    "\n",
    "# Focused tuning on 3 hyperparameters\n",
    "sweep_configs = [\n",
    "    {\"learning_rate\": lr, \"weight_decay\": wd, \"warmup_ratio\": wr}\n",
    "    for lr in [1e-5, 2e-5, 3e-5, 5e-5]\n",
    "    for wd in [0.0, 0.01, 0.05, 0.1]\n",
    "    for wr in [0.0, 0.05, 0.1, 0.2]\n",
    "]\n",
    "\n",
    "# Optional: Limit to 12 runs for Week 2\n",
    "random.seed(42)\n",
    "sweep_configs = random.sample(sweep_configs, 12)\n",
    "\n",
    "# ============================================\n",
    "# Run Experiments\n",
    "# ============================================\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "for cfg in sweep_configs:\n",
    "    full_cfg = default_config.copy()\n",
    "    full_cfg.update(cfg)\n",
    "    train_model(full_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcc423",
   "metadata": {},
   "source": [
    "### Manual Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e6487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_run_details(entity, project_name, num_runs=2):\n",
    "    \"\"\"\n",
    "    Deep inspection of run structure to find where metrics are stored.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "\n",
    "    print(f\"Total runs in project: {len(runs)}\\n\")\n",
    "\n",
    "    for i, run in enumerate(runs):\n",
    "        if i >= num_runs:\n",
    "            break\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(f\"RUN {i+1}: {run.name}\")\n",
    "        print(f\"ID: {run.id}\")\n",
    "        print(f\"State: {run.state}\")\n",
    "        print(f\"Created: {run.created_at}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\n--- CONFIG ---\")\n",
    "        print(f\"Type: {type(run.config)}\")\n",
    "        if hasattr(run.config, '__dict__'):\n",
    "            print(\"Config attributes:\", dir(run.config))\n",
    "        try:\n",
    "            config_dict = dict(run.config) if hasattr(run.config, '__iter__') else {}\n",
    "            for key, val in config_dict.items():\n",
    "                if not key.startswith('_'):\n",
    "                    print(f\"  {key}: {val}\")\n",
    "        except:\n",
    "            print(\"  Could not extract config\")\n",
    "\n",
    "        print(\"\\n--- SUMMARY ---\")\n",
    "        print(f\"Type: {type(run.summary)}\")\n",
    "        if hasattr(run.summary, '__dict__'):\n",
    "            print(\"Summary attributes:\", [a for a in dir(run.summary) if not a.startswith('_')])\n",
    "\n",
    "        try:\n",
    "            if hasattr(run.summary, '_json_dict'):\n",
    "                print(\"\\nSummary _json_dict:\")\n",
    "                for key, val in run.summary._json_dict.items():\n",
    "                    print(f\"  {key}: {val}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if hasattr(run.summary, 'keys'):\n",
    "                summary_keys = list(run.summary.keys())\n",
    "                print(f\"\\nSummary keys: {summary_keys}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"\\n--- HISTORY (last 5 logged steps) ---\")\n",
    "        try:\n",
    "            history = run.history(samples=5)\n",
    "            print(f\"History shape: {history.shape}\")\n",
    "            print(f\"History columns: {list(history.columns)}\")\n",
    "            print(\"\\nLast 5 rows:\")\n",
    "            print(history)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get history: {e}\")\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def get_all_runs_with_history(entity, project_name):\n",
    "    \"\"\"\n",
    "    Get final metrics from run history (more reliable than summary).\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for run in runs:\n",
    "        row = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            \"state\": run.state,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            config_dict = dict(run.config) if hasattr(run.config, '__iter__') else {}\n",
    "            for key, val in config_dict.items():\n",
    "                if not key.startswith('_'):\n",
    "                    row[f\"config_{key}\"] = val\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            history = run.history()\n",
    "            if len(history) > 0:\n",
    "                last_row = history.iloc[-1]\n",
    "\n",
    "                for col in history.columns:\n",
    "                    if not col.startswith('_'):\n",
    "                        row[col] = last_row[col]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get history for run {run.name}: {e}\")\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_metrics_summary(entity, project_name):\n",
    "    \"\"\"\n",
    "    Extract key metrics focusing on eval metrics from final evaluation.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(f\"{entity}/{project_name}\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for run in runs:\n",
    "        row = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            \"state\": run.state,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            config_dict = dict(run.config) if hasattr(run.config, '__iter__') else {}\n",
    "            row[\"learning_rate\"] = config_dict.get(\"learning_rate\", None)\n",
    "            row[\"weight_decay\"] = config_dict.get(\"weight_decay\", None)\n",
    "            row[\"warmup_ratio\"] = config_dict.get(\"warmup_ratio\", None)\n",
    "            row[\"classifier_dropout\"] = config_dict.get(\"classifier_dropout\", None)\n",
    "            row[\"per_device_train_batch_size\"] = config_dict.get(\"per_device_train_batch_size\", None)\n",
    "            row[\"optimizer_type\"] = config_dict.get(\"optimizer_type\", None)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            history = run.history()\n",
    "            if len(history) > 0:\n",
    "                eval_rows = history[history['eval_accuracy'].notna()]\n",
    "\n",
    "                if len(eval_rows) > 0:\n",
    "                    final_eval = eval_rows.iloc[-1]\n",
    "                    row[\"eval_accuracy\"] = final_eval.get(\"eval_accuracy\", None)\n",
    "                    row[\"eval_f1\"] = final_eval.get(\"eval_f1\", None)\n",
    "                    row[\"eval_loss\"] = final_eval.get(\"eval_loss\", None)\n",
    "\n",
    "                train_rows = history[history['train_loss'].notna()]\n",
    "                if len(train_rows) > 0:\n",
    "                    final_train = train_rows.iloc[-1]\n",
    "                    row[\"final_train_loss\"] = final_train.get(\"train_loss\", None)\n",
    "                    row[\"final_learning_rate\"] = final_train.get(\"train_learning_rate\", None)\n",
    "                    row[\"final_grad_norm\"] = final_train.get(\"train_grad_norm\", None)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process run {run.name}: {e}\")\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ENTITY = \"janick-steffen-hslu\"\n",
    "    PROJECT = \"MLOPS_p1_distilbert\"\n",
    "\n",
    "    print(\"STEP 1: Deep inspection of run structure\")\n",
    "    print(\"=\"*80)\n",
    "    inspect_run_details(ENTITY, PROJECT, num_runs=1)\n",
    "\n",
    "    print(\"\\n\\nSTEP 2: Get all metrics from history\")\n",
    "    print(\"=\"*80)\n",
    "    df = get_metrics_summary(ENTITY, PROJECT)\n",
    "\n",
    "    print(f\"\\nFound {len(df)} runs\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "    print(\"\\n\\nAll runs sorted by eval_accuracy:\")\n",
    "    df_sorted = df.sort_values(\"eval_accuracy\", ascending=False)\n",
    "\n",
    "    cols_to_show = [c for c in [\n",
    "        \"run_name\", \"eval_accuracy\", \"eval_f1\",\n",
    "        \"learning_rate\", \"weight_decay\", \"warmup_ratio\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    print(df_sorted[cols_to_show])\n",
    "\n",
    "    df.to_csv(\"wandb_runs_summary.csv\", index=False)\n",
    "    print(\"\\n\\nExported to wandb_runs_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef1d66",
   "metadata": {},
   "source": [
    "#### Optimized by claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc5693",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimal Hyperparameter Configuration\n",
    "Based on 12 manual tuning runs in Week 2\n",
    "\n",
    "Analysis Summary:\n",
    "- Learning Rate 3e-5: Best average performance (85.13%) and highest max (85.78%)\n",
    "- Weight Decay 0.1: Good regularization, prevents overfitting\n",
    "- Warmup Ratio 0.2: Stable training with gradual LR warmup\n",
    "\n",
    "Best Result: 85.78% validation accuracy, 90.17% F1, 0.3456 loss\n",
    "\"\"\"\n",
    "\n",
    "# Use this configuration for your next training run\n",
    "hyperparameter_config = {\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    \"optimizer_type\": \"adamw_torch\",\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"classifier_dropout\": 0.1,\n",
    "}\n",
    "\n",
    "# Run this configuration\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize wandb\n",
    "    wandb.login()\n",
    "\n",
    "    # Train with optimal config\n",
    "    print(\"Training with optimal hyperparameters...\")\n",
    "    print(f\"LR: {optimal_config['learning_rate']}\")\n",
    "    print(f\"Weight Decay: {optimal_config['weight_decay']}\")\n",
    "    print(f\"Warmup Ratio: {optimal_config['warmup_ratio']}\")\n",
    "\n",
    "    results = train_model(optimal_config)\n",
    "    print(f\"\\nResults: {results}\")\n",
    "\n",
    "\n",
    "# For further fine-tuning (Week 3 or bonus), explore around the optimum:\n",
    "fine_tuning_ranges = {\n",
    "    \"learning_rate\": [2.5e-5, 3e-5, 3.5e-5, 4e-5],\n",
    "    \"weight_decay\": [0.08, 0.09, 0.1, 0.11, 0.12],\n",
    "    \"warmup_ratio\": [0.15, 0.18, 0.2, 0.22, 0.25],\n",
    "}\n",
    "\n",
    "print(\"\\nFor Week 3 automatic optimization, use these ranges:\")\n",
    "print(fine_tuning_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885e7f2",
   "metadata": {},
   "source": [
    "# Week 3 - automated optimal config with Bayesian, grid and random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fd126",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MLOps Project 1 - Week 3: Automatic Hyperparameter Optimization\n",
    "# Using Wandb Sweeps with Bayesian Optimization\n",
    "# ============================================\n",
    "\n",
    "# Run this first in Colab\n",
    "# !pip install -q transformers datasets evaluate wandb\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, AutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Initialize Weights & Biases ---\n",
    "wandb.login()\n",
    "PROJECT_NAME = \"MLOPS_p1_distilbert\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- Metric ---\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# --- Training Function for Sweep ---\n",
    "def train_sweep():\n",
    "    \"\"\"\n",
    "    Training function called by wandb sweep agent.\n",
    "    Uses hyperparameters from wandb.config automatically.\n",
    "    \"\"\"\n",
    "    # Create descriptive run name based on hyperparameters\n",
    "    run_name = None\n",
    "\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Create descriptive run name similar to manual runs\n",
    "        run_name = (f\"lr{config.learning_rate:.0e}_\"\n",
    "                   f\"wd{config.weight_decay}_\"\n",
    "                   f\"wr{config.warmup_ratio}_\"\n",
    "                   f\"sweep_bayesian\")\n",
    "        run.name = run_name\n",
    "\n",
    "        set_seed(42)\n",
    "\n",
    "        # --- Load data ---\n",
    "        dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "        encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        # --- Model setup ---\n",
    "        model_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model_config.num_labels = 2\n",
    "        model_config.hidden_dropout_prob = config.classifier_dropout\n",
    "\n",
    "        # Note: Warning about uninitialized weights is expected and normal\n",
    "        # The classification head is randomly initialized and will be trained\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\",\n",
    "            config=model_config,\n",
    "        )\n",
    "\n",
    "        # --- Training Arguments ---\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            num_train_epochs=3,\n",
    "            lr_scheduler_type=config.lr_scheduler_type,\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            optim=config.optimizer_type,\n",
    "            logging_steps=50,\n",
    "            report_to=\"wandb\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            seed=42,\n",
    "            data_seed=42,\n",
    "        )\n",
    "\n",
    "        # --- Trainer ---\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=encoded_dataset[\"train\"],\n",
    "            eval_dataset=encoded_dataset[\"validation\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        # --- Train & Evaluate ---\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        wandb.log({\n",
    "            \"final_eval_accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"final_eval_f1\": eval_results[\"eval_f1\"],\n",
    "            \"final_eval_loss\": eval_results[\"eval_loss\"],\n",
    "        })\n",
    "\n",
    "# ============================================\n",
    "# Sweep Configuration\n",
    "# ============================================\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"week3_bayesian_sweep\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"eval_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        # Focus on the 3 most important hyperparameters\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 2.5e-5,\n",
    "            \"max\": 4e-5\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.08,\n",
    "            \"max\": 0.12\n",
    "        },\n",
    "        \"warmup_ratio\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.15,\n",
    "            \"max\": 0.25\n",
    "        },\n",
    "        # Fixed parameters based on Week 2 findings\n",
    "        \"optimizer_type\": {\n",
    "            \"value\": \"adamw_torch\"\n",
    "        },\n",
    "        \"lr_scheduler_type\": {\n",
    "            \"value\": \"linear\"\n",
    "        },\n",
    "        \"per_device_train_batch_size\": {\n",
    "            \"value\": 16\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": {\n",
    "            \"value\": 1\n",
    "        },\n",
    "        \"classifier_dropout\": {\n",
    "            \"value\": 0.1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# Initialize and Run Sweep\n",
    "# ============================================\n",
    "\n",
    "# Create sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
    "\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"Running 12 sweep runs with Bayesian optimization...\")\n",
    "print(f\"Searching around optimal region:\")\n",
    "print(f\"  Learning Rate: [2.5e-5, 4e-5]\")\n",
    "print(f\"  Weight Decay: [0.08, 0.12]\")\n",
    "print(f\"  Warmup Ratio: [0.15, 0.25]\")\n",
    "\n",
    "# Run sweep agent (12 runs)\n",
    "wandb.agent(sweep_id, function=train_sweep, count=12)\n",
    "\n",
    "print(\"\\nSweep completed!\")\n",
    "print(f\"View results at: https://wandb.ai/{wandb.api.default_entity}/{PROJECT_NAME}/sweeps/{sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148cd64e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Quick Check - Verify Your Sweep is Ready\n",
    "# Run this BEFORE the full analysis\n",
    "# ============================================\n",
    "\n",
    "import wandb\n",
    "\n",
    "def quick_check_sweep(entity, project_name, sweep_id):\n",
    "    \"\"\"\n",
    "    Quick sanity check to verify sweep is complete and has data.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"QUICK SWEEP CHECK\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        sweep = api.sweep(f\"{entity}/{project_name}/{sweep_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: Could not find sweep!\")\n",
    "        print(f\"   {e}\")\n",
    "        print(f\"\\nCheck:\")\n",
    "        print(f\"  - ENTITY: {entity}\")\n",
    "        print(f\"  - PROJECT: {project_name}\")\n",
    "        print(f\"  - SWEEP_ID: {sweep_id}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\n‚úì Sweep found: {sweep.name}\")\n",
    "    print(f\"  State: {sweep.state}\")\n",
    "    print(f\"  URL: {sweep.url}\")\n",
    "\n",
    "    runs = list(sweep.runs)\n",
    "    print(f\"\\n‚úì Number of runs: {len(runs)}\")\n",
    "\n",
    "    if len(runs) == 0:\n",
    "        print(\"\\n‚ùå ERROR: No runs in this sweep!\")\n",
    "        return False\n",
    "\n",
    "    # Check run states\n",
    "    finished = sum(1 for r in runs if r.state == \"finished\")\n",
    "    running = sum(1 for r in runs if r.state == \"running\")\n",
    "    failed = sum(1 for r in runs if r.state == \"failed\")\n",
    "    crashed = sum(1 for r in runs if r.state == \"crashed\")\n",
    "\n",
    "    print(f\"\\nRun states:\")\n",
    "    print(f\"  ‚úì Finished: {finished}\")\n",
    "    if running > 0:\n",
    "        print(f\"  ‚è≥ Running: {running}\")\n",
    "    if failed > 0:\n",
    "        print(f\"  ‚ùå Failed: {failed}\")\n",
    "    if crashed > 0:\n",
    "        print(f\"  ‚ùå Crashed: {crashed}\")\n",
    "\n",
    "    if finished == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: No finished runs yet!\")\n",
    "        print(\"   Wait for runs to complete before analysis.\")\n",
    "        return False\n",
    "\n",
    "    # Check first finished run for data\n",
    "    finished_runs = [r for r in runs if r.state == \"finished\"]\n",
    "    first_run = finished_runs[0]\n",
    "\n",
    "    print(f\"\\n‚úì Checking first finished run: {first_run.name}\")\n",
    "\n",
    "    try:\n",
    "        history = first_run.history()\n",
    "\n",
    "        # Check for eval metrics\n",
    "        eval_cols = [c for c in history.columns if 'eval' in c.lower() and 'accuracy' in c.lower()]\n",
    "\n",
    "        if len(eval_cols) == 0:\n",
    "            print(\"\\n‚ùå ERROR: No eval accuracy column found!\")\n",
    "            print(f\"   Available columns: {[c for c in history.columns if not c.startswith('_')][:10]}\")\n",
    "            return False\n",
    "\n",
    "        acc_col = eval_cols[0]\n",
    "        print(f\"\\n‚úì Found accuracy column: '{acc_col}'\")\n",
    "\n",
    "        # Check if it has data\n",
    "        acc_data = history[acc_col].dropna()\n",
    "        if len(acc_data) == 0:\n",
    "            print(f\"\\n‚ùå ERROR: '{acc_col}' column is empty!\")\n",
    "            return False\n",
    "\n",
    "        final_acc = acc_data.iloc[-1]\n",
    "        print(f\"  Final accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "\n",
    "        # Check for other eval metrics\n",
    "        f1_cols = [c for c in history.columns if 'eval' in c.lower() and 'f1' in c.lower()]\n",
    "        if len(f1_cols) > 0:\n",
    "            f1_col = f1_cols[0]\n",
    "            final_f1 = history[f1_col].dropna().iloc[-1]\n",
    "            print(f\"  Final F1: {final_f1:.4f} ({final_f1*100:.2f}%)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR reading run data: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Check config\n",
    "    print(f\"\\n‚úì Checking config...\")\n",
    "    try:\n",
    "        config = dict(first_run.config) if hasattr(first_run.config, '__iter__') else {}\n",
    "\n",
    "        if 'learning_rate' in config:\n",
    "            print(f\"  LR: {config['learning_rate']}\")\n",
    "            print(f\"  WD: {config.get('weight_decay', 'N/A')}\")\n",
    "            print(f\"  WR: {config.get('warmup_ratio', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Config not in standard format\")\n",
    "            print(\"     (Will extract from run names instead)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Could not read config: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ SWEEP READY FOR ANALYSIS!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nYou can now run:\")\n",
    "    print(\"  python analyze_sweep_results.py\")\n",
    "    print(f\"\\nOr use SWEEP_ID: {sweep_id}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # UPDATE THESE VALUES\n",
    "    ENTITY = \"janick-steffen-hslu\"\n",
    "    PROJECT = \"MLOPS_p1_distilbert\"\n",
    "    SWEEP_ID = \"425e5baw\"\n",
    "\n",
    "    print(\"Checking sweep readiness...\")\n",
    "    print(f\"Entity: {ENTITY}\")\n",
    "    print(f\"Project: {PROJECT}\")\n",
    "    print(f\"Sweep ID: {SWEEP_ID}\\n\")\n",
    "\n",
    "    ready = quick_check_sweep(ENTITY, PROJECT, SWEEP_ID)\n",
    "\n",
    "    if not ready:\n",
    "        print(\"\\n‚ö†Ô∏è  Sweep is not ready for analysis yet.\")\n",
    "        print(\"   Fix the issues above and try again.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
